{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51a2cf8d",
   "metadata": {},
   "source": [
    "# Nightjars Object Detection and Distance Estimation\n",
    "\n",
    "This notebook combines object detection using YOLOv8 and distance estimation for detected objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106825f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7da896",
   "metadata": {},
   "source": [
    "## Constants and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f9e07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance constants\n",
    "KNOWN_DISTANCE = 45  # INCHES\n",
    "PERSON_WIDTH = 16  # INCHES\n",
    "MOBILE_WIDTH = 3.0  # INCHES\n",
    "CHAIR_WIDTH = 20.0  # INCHES\n",
    "LAPTOP_WIDTH = 12  # INCHES\n",
    "\n",
    "# Object detector constants\n",
    "CONFIDENCE_THRESHOLD = 0.4\n",
    "NMS_THRESHOLD = 0.3\n",
    "\n",
    "# Colors for visualization\n",
    "COLORS = [(151, 157, 255),(56, 56, 255), (31, 112, 255), (29, 178, 255), (49, 210, 207), \n",
    "          (10, 249, 72), (23, 204, 146), (134, 219, 61), (52, 147, 26), (187, 212, 0),\n",
    "          (168, 153, 44), (255, 194, 0), (147, 69, 52), (255, 115, 100), (236, 24, 0),\n",
    "          (255, 56, 132), (133, 0, 82), (255, 56, 203), (200, 149, 255), (199, 55, 255)]\n",
    "\n",
    "WHITE = (255, 255, 255)\n",
    "BLACK = (0, 0, 0)\n",
    "FONTS = cv2.FONT_HERSHEY_PLAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7119bdd7",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ccbcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_length_finder(measured_distance, real_width, width_in_rf):\n",
    "    \"\"\"Calculate focal length using known distance and object width\"\"\"\n",
    "    return (width_in_rf * measured_distance) / real_width\n",
    "\n",
    "def distance_finder(focal_length, real_object_width, width_in_frame):\n",
    "    \"\"\"Calculate distance using focal length and object width\"\"\"\n",
    "    return (real_object_width * focal_length) / width_in_frame\n",
    "\n",
    "def object_detector(image, model):\n",
    "    \"\"\"Detect objects in the image using YOLOv8 model\"\"\"\n",
    "    results = model(image)\n",
    "    data_list = []\n",
    "    detected_objects = {}\n",
    "    \n",
    "    for result in results:\n",
    "        for box, score, class_id in zip(result.boxes.xyxy, result.boxes.conf, result.boxes.cls):\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "            height, width, _ = image.shape\n",
    "            \n",
    "            if (center_x, center_y) in detected_objects:\n",
    "                continue\n",
    "            else:\n",
    "                detected_objects[(center_x, center_y)] = True\n",
    "            \n",
    "            W_pos = \"left\" if center_x <= width / 3 else \"center\" if center_x <= 2 * width / 3 else \"right\"\n",
    "            H_pos = \"top\" if center_y <= height / 3 else \"mid\" if center_y <= 2 * height / 3 else \"bottom\"\n",
    "            \n",
    "            color = COLORS[int(class_id) % len(COLORS)]\n",
    "            label = f\"{model.names[int(class_id)]} : {score:.2f}\"\n",
    "            \n",
    "            cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(image, label, (x1, y1 - 10), FONTS, 0.5, color, 2)\n",
    "            \n",
    "            if int(class_id) in [0, 67, 56, 72]:  # person, mobile, chair, laptop\n",
    "                data_list.append([model.names[int(class_id)], x2 - x1, (x1, y1 - 2), W_pos, H_pos])\n",
    "    \n",
    "    return data_list, image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0ebe50",
   "metadata": {},
   "source": [
    "## Load Model and Reference Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ab383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLOv8 model\n",
    "model = YOLO('yolov8xcdark.pt')\n",
    "\n",
    "# Load reference images\n",
    "ref_person = cv2.imread('ReferenceImages/image14.png')\n",
    "ref_mobile = cv2.imread('ReferenceImages/image4.png')\n",
    "ref_chair = cv2.imread('ReferenceImages/image22.png')\n",
    "ref_laptop = cv2.imread('ReferenceImages/image2.png')\n",
    "\n",
    "# Get reference widths\n",
    "person_data, _ = object_detector(ref_person, model)\n",
    "person_width_in_rf = person_data[0][1]\n",
    "\n",
    "mobile_data, _ = object_detector(ref_mobile, model)\n",
    "mobile_width_in_rf = mobile_data[0][1]\n",
    "\n",
    "chair_data, _ = object_detector(ref_chair, model)\n",
    "chair_width_in_rf = chair_data[0][1]\n",
    "\n",
    "# Calculate focal lengths\n",
    "focal_person = focal_length_finder(KNOWN_DISTANCE, PERSON_WIDTH, person_width_in_rf)\n",
    "focal_mobile = focal_length_finder(KNOWN_DISTANCE, MOBILE_WIDTH, mobile_width_in_rf)\n",
    "focal_chair = focal_length_finder(KNOWN_DISTANCE, CHAIR_WIDTH, chair_width_in_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab43209",
   "metadata": {},
   "source": [
    "## Process Image Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8531df65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image_path):\n",
    "    \"\"\"Process an image and return the annotated image with distance information\"\"\"\n",
    "    # Read image\n",
    "    frame = cv2.imread(image_path)\n",
    "    if frame is None:\n",
    "        raise ValueError(f\"Could not read image at {image_path}\")\n",
    "    \n",
    "    # Detect objects\n",
    "    data, annotated_frame = object_detector(frame, model)\n",
    "    \n",
    "    # Process each detected object\n",
    "    for d in data:\n",
    "        if d[0] == 'person':\n",
    "            distance = distance_finder(focal_person, PERSON_WIDTH, d[1])\n",
    "        elif d[0] == 'cell phone':\n",
    "            distance = distance_finder(focal_mobile, MOBILE_WIDTH, d[1])\n",
    "        elif d[0] == 'chair':\n",
    "            distance = distance_finder(focal_chair, CHAIR_WIDTH, d[1])\n",
    "        \n",
    "        x, y = d[2]\n",
    "        text1, text2 = d[3], d[4]\n",
    "        \n",
    "        # Add distance information\n",
    "        cv2.rectangle(annotated_frame, (x+2, y+4), (x + 150, y + 20), BLACK, -1)\n",
    "        cv2.putText(annotated_frame, f'Distance: {round(distance, 2)} inches', \n",
    "                    (x + 7, y + 17), FONTS, 0.58, WHITE, 1)\n",
    "        \n",
    "        # Generate guidance text\n",
    "        if distance > 100:\n",
    "            guidance = \"Get closer\"\n",
    "        elif 50 < round(distance) <= 100 and text2 == \"mid\":\n",
    "            guidance = \"Go straight\"\n",
    "        else:\n",
    "            guidance = f\"{d[0]} {int(round(distance))} inches, take left or right\"\n",
    "            \n",
    "        print(f\"Object: {d[0]}, Distance: {round(distance, 2)} inches, Guidance: {guidance}\")\n",
    "    \n",
    "    return annotated_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec8cc11",
   "metadata": {},
   "source": [
    "## Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da73c4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a test image\n",
    "test_image_path = 'test_images/test1.jpg'  # Replace with your test image path\n",
    "result_image = process_image(test_image_path)\n",
    "\n",
    "# Display the result\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(cv2.cvtColor(result_image, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15ec680",
   "metadata": {},
   "source": [
    "## Video Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e63974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path, output_path):\n",
    "    \"\"\"Process a video file and save the annotated output\"\"\"\n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Could not open video at {video_path}\")\n",
    "    \n",
    "    # Get video properties\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    \n",
    "    # Create video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # Process frame\n",
    "        data, annotated_frame = object_detector(frame, model)\n",
    "        \n",
    "        # Add distance information for each detected object\n",
    "        for d in data:\n",
    "            if d[0] == 'person':\n",
    "                distance = distance_finder(focal_person, PERSON_WIDTH, d[1])\n",
    "            elif d[0] == 'cell phone':\n",
    "                distance = distance_finder(focal_mobile, MOBILE_WIDTH, d[1])\n",
    "            elif d[0] == 'chair':\n",
    "                distance = distance_finder(focal_chair, CHAIR_WIDTH, d[1])\n",
    "            \n",
    "            x, y = d[2]\n",
    "            cv2.rectangle(annotated_frame, (x+2, y+4), (x + 150, y + 20), BLACK, -1)\n",
    "            cv2.putText(annotated_frame, f'Distance: {round(distance, 2)} inches', \n",
    "                        (x + 7, y + 17), FONTS, 0.58, WHITE, 1)\n",
    "        \n",
    "        # Write frame to output video\n",
    "        out.write(annotated_frame)\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    \n",
    "    print(f\"Video processing complete. Output saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf6030d",
   "metadata": {},
   "source": [
    "## Example Video Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e72d677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a test video\n",
    "test_video_path = 'test_videos/test1.mp4'  # Replace with your test video path\n",
    "output_video_path = 'output_videos/processed_test1.mp4'\n",
    "process_video(test_video_path, output_video_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
